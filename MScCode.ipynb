{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MScCode",
      "provenance": [],
      "collapsed_sections": [
        "8jLxiAwpHUrt",
        "5aFRMDnxG3yZ",
        "Gca7rp-1Gimq",
        "w1w9SOqkf5vE",
        "yJtV04hjOid6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelleaeh/ADissertation/blob/master/MScCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcws6GC7s8aT",
        "colab_type": "text"
      },
      "source": [
        "**Project:** MSc in Robotics and Intelligent Systems Dissertation\n",
        "\n",
        "**Author:** Michelle Alejandra Espinosa Hernandez\n",
        "\n",
        "**Student registration number:** 1900964\n",
        "\n",
        "**Student PRID:** ESPIN62803\n",
        "\n",
        "**Date:** September 2020 \n",
        "\n",
        "**Purpose:** Obtain classification accuracy among different types of sensors and different data processing steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jLxiAwpHUrt",
        "colab_type": "text"
      },
      "source": [
        "# **Data description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH79ogCWrzbt",
        "colab_type": "text"
      },
      "source": [
        "**Myo armband dataset from https://data.mendeley.com/datasets/wgswcr8z24/2**\n",
        "\n",
        "\n",
        "The dataset consits of .csv files collected from two Myo armbands. The format of the files are [word_name]_[id]. The ‘word_name’ is the English translation of the American Sign Language word used and the ‘id’ is a unique identifier. The .zip for each of the above links has sub-folders for each User.\n",
        "\n",
        "Each file has 50 columns. They represent a sub-sampled data collection from two Myo devices worn on left and right hands of the signer. The first column is the ‘Counter’ that goes from 1 to 50.\n",
        "\n",
        "The following columns are of the format: [Sensor][pod/direction][left/right]. For instance the EMG reading for the first EMG pod (out of 8) on the left hand would be called EMG0R and the accelerometer reading for the Z axis on the left hand would be called: AXL\n",
        "\n",
        "If you use this dataset please cite the following papers:\n",
        "\n",
        "@inproceedings{paudyal2016sceptre,\n",
        "title={Sceptre: a pervasive, non-invasive, and programmable gesture recognition technology},\n",
        "author={Paudyal, Prajwal and Banerjee, Ayan and Gupta, Sandeep KS},\n",
        "booktitle={Proceedings of the 21st International Conference on Intelligent User Interfaces},\n",
        "pages={282--293},\n",
        "year={2016},\n",
        "organization={ACM}\n",
        "}\n",
        "\n",
        "@inproceedings{paudyal2017dyfav,\n",
        "title={Dyfav: Dynamic feature selection and voting for real-time recognition of fingerspelled alphabet using wearables},\n",
        "author={Paudyal, Prajwal and Lee, Junghyo and Banerjee, Ayan and Gupta, Sandeep KS},\n",
        "booktitle={Proceedings of the 22nd International Conference on Intelligent User Interfaces},\n",
        "pages={457--467},\n",
        "year={2017},\n",
        "organization={ACM}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHNKWJHfhpMy",
        "colab_type": "text"
      },
      "source": [
        "**Frequency:**\n",
        "\n",
        "50Hz sampling rate\n",
        "\n",
        "**Words:**\n",
        "\n",
        "*36 total words*\n",
        "\n",
        "allmorning, bird, blue, cantsleep, cat, colrunnynose, continuouslyforanhour, cost, day, dollar, everymorning, everynight, gold, goodnight, happy, headache, home, horse, hot, hurt, itching, large, mom, monthly, notfeelgood, orange, pizza, please, shirt, soreness, swelling, takeliquidmedicine, thatsterrible, tired, upsetstomach, wash\n",
        "\n",
        "\n",
        "**Filenames:**\n",
        "\n",
        "*849 total files*\n",
        "\n",
        "(word)_(user#)(try#)\n",
        "\n",
        "\n",
        "**Columns of files:**\n",
        "\n",
        "Counter  (1 -> 50)\n",
        "\n",
        "EMG0L -> EMG7L  (EMG sensor readings)\n",
        "\n",
        "AXL, AYL, AZL  (accelerometer readings)\n",
        "\n",
        "GXL, GYL, GZL  (gyroscope readings)\n",
        "\n",
        "ORL, OPL, OYL  (magnetometer readings?)\n",
        "\n",
        "EMG0R -> EMG7R  (EMG sensor readings)\n",
        "\n",
        "AXR, AYR, AZR  (accelerometer readings)\n",
        "\n",
        "GXR, GYR, GZR  (gyroscope readings)\n",
        "\n",
        "ORR, OPR, OYR  (magnetometer readings?)\n",
        "\n",
        "features=['EMG0L', 'EMG1L', 'EMG2L', 'EMG3L', 'EMG4L', 'EMG5L', 'EMG6L', 'EMG7L', 'AXL', 'AYL', 'AZL', 'GXL', 'GYL', 'GZL', 'ORL', 'OPL', 'OYL', 'EMG0R', 'EMG1R', 'EMG2R', 'EMG3R', 'EMG4R', 'EMG5R', 'EMG6R', 'EMG7R', 'AXR', 'AYR', 'AZR', 'GXR', 'GYR', 'GZR', 'ORR', 'OPR', 'OYR']\n",
        "\n",
        "\n",
        "**Size of files:**\n",
        "\n",
        "All files are 50 rows x 35 columns except continuouslyforanhour_22.csv, headache_52.csv, home_61.csv, and mom_82.csv which are 101 rows x 35 columns\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Combine files\n",
        "2. Normalize or standardize matrix\n",
        "3. Apply Butterworth\n",
        "4. Apply PCA\n",
        "5. Input to SVM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aFRMDnxG3yZ",
        "colab_type": "text"
      },
      "source": [
        "# **Variable definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oweA9Tcyf_I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numreps=17      # Number of epochs per word\n",
        "num_trials=10  # Number of runs of cross validation"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gca7rp-1Gimq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **1. Preparation of data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXI1GgkfFdai",
        "colab_type": "text"
      },
      "source": [
        "**1.1. Start up and initialization of variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TnTycFtrpHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import random\n",
        "import scipy as sp\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy import signal\n",
        "from scipy.io import loadmat\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Start timer\n",
        "starttime = time.time()\n",
        "\n",
        "# Eliminate warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Extract all files from zip\n",
        "with ZipFile(\"/content/2MyoASL.zip\", 'r') as zip:\n",
        "  zip.extractall()\n",
        "\n",
        "# Division of sensors\n",
        "emg=['EMG0L', 'EMG1L', 'EMG2L', 'EMG3L', 'EMG4L', 'EMG5L', 'EMG6L', 'EMG7L', \n",
        "     'EMG0R', 'EMG1R', 'EMG2R', 'EMG3R', 'EMG4R', 'EMG5R', 'EMG6R', 'EMG7R']\n",
        "acc=['AXL', 'AYL', 'AZL', 'AXR', 'AYR', 'AZR']\n",
        "gyro=['GXL', 'GYL', 'GZL', 'GXR', 'GYR', 'GZR']\n",
        "ori=['ORL', 'OPL', 'OYL', 'ORR', 'OPR', 'OYR']\n",
        "colnames=emg[:8]+acc[:3]+gyro[:3]+ori[:3]+emg[8:]+acc[3:]+gyro[3:]+ori[3:]\n",
        "\n",
        "# Words\n",
        "words=['allmorning', 'bird', 'blue', 'cantsleep', 'cat', 'coldrunnynose', 'continuouslyforanhour', 'cost', 'day', \n",
        "       'dollar', 'everymorning', 'everynight', 'gold', 'goodnight', 'happy', 'headache', 'home', 'horse', 'hot', \n",
        "       'hurt', 'itching', 'large', 'mom', 'monthly', 'notfeelgood', 'orange', 'pizza', 'please', 'shirt', \n",
        "       'soreness', 'swelling', 'takeliquidmedicine', 'thatsterrible', 'tired', 'upsetstomach', 'wash']\n",
        "\n",
        "# Generation of matrices\n",
        "### Combinations of sensors (E=emg=3, A=acc=5, G=gyro=7, O=ori=11)\n",
        "comb=['E', 'A', 'G', 'O', 'EA', 'EG', 'EO', 'AG', 'AO', 'GO', 'EAG', 'EAO', 'EGO', 'AGO', 'EAGO'] \n",
        "products=[3, 5, 7, 11, 15, 21, 33, 35, 55, 77, 105, 165, 231, 385, 1155] \n",
        "### Combinations of steps (N=Normalization=3, S=Standardization=5, B=Butterworth=7, P=PCA=11, V=SVM=13)\n",
        "nsteps=['V', 'NV', 'SV', 'BV', 'PV', 'NBV', 'NPV', 'SBV', 'SPV', 'BPV', 'NBPV', 'SBPV'] \n",
        "steps=[13, 39, 65, 91, 143, 273, 429, 455, 715, 1001, 3003, 5005] \n",
        "### Placeholders\n",
        "fresults=np.zeros((len(steps),len(products)))\n",
        "params=np.zeros((len(steps),len(products)))\n",
        "headers=np.empty(1701, dtype=object)\n",
        "lengths=np.zeros(849, dtype=int)\n",
        "repsum=np.zeros(37,dtype=int)\n",
        "reps=np.zeros(36,dtype=int)\n",
        "features=np.zeros(15)\n",
        "target=np.zeros(15)\n",
        "fn=np.arange(1701)\n",
        "matrix=np.zeros(1)\n",
        "stanproducts=[]\n",
        "normproducts=[]\n",
        "regproducts=[]\n",
        "cmatrix=[]\n",
        "\n",
        "# Initiation of counters\n",
        "wordnum=-1\n",
        "counter=-1\n",
        "rownum=-1\n",
        "start=0\n",
        "num=0\n",
        "n=0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAKRziACFoe0",
        "colab_type": "text"
      },
      "source": [
        "**1.2. Combine all files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQFtofv7yG7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3f13a952-3835-41f1-9564-40add465af36"
      },
      "source": [
        "for w in words:\n",
        "  repcount=0\n",
        "  wordnum+=1\n",
        "  for i in range (10, 120):\n",
        "    path='/content/2MyoASL/' + w + '_' + str(i) + '.csv'\n",
        "    if os.path.exists(path)==True:\n",
        "      counter+=1\n",
        "      repcount+=1\n",
        "      trial=pd.read_csv(path)\n",
        "      trial.reset_index(drop=True)\n",
        "      \n",
        "      # Assign word number to each row and make data horizontal\n",
        "      row=np.zeros(1)\n",
        "      for t in range(35):\n",
        "        if t==0:\n",
        "          row[0]=wordnum\n",
        "        else:\n",
        "          sensor=trial.iloc[0:50,t].values\n",
        "          sensor.reshape([1,50])\n",
        "          row=np.concatenate((row, sensor))\n",
        "      prev=row\n",
        "      \n",
        "      # Combine all trials\n",
        "      if counter==0:\n",
        "        matrix=prev\n",
        "      else:\n",
        "        matrix=np.concatenate([matrix,prev])\n",
        "\n",
        "  reps[wordnum]=repcount\n",
        "  if wordnum>0:\n",
        "    repsum[wordnum]=reps[wordnum-1]+repsum[wordnum-1]\n",
        "    repsum[36]=849\n",
        "\n",
        "# Create header name array\n",
        "headers[0]='Word'\n",
        "for c in colnames:\n",
        "  for t in range(50):\n",
        "    num+=1\n",
        "    headers[num]=c\n",
        "\n",
        "# Give format to final matrix \n",
        "matrix=matrix.reshape([849,1701])\n",
        "matrix=pd.DataFrame(matrix, columns=headers)\n",
        "matrix=pd.concat([matrix['Word'],matrix[emg],matrix[acc],matrix[gyro],matrix[ori]],axis=1)\n",
        "exec(\"matrix.to_csv(path_or_buf='/content/complete_matrix_'+str(numreps)+'.csv')\")\n",
        "print('Reps of each word:',reps)\n",
        "print('Cummulative reps:',repsum)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reps of each word: [19 24 32 20 24 20 19 31 24 29 20 21 23 33 34 18 27 24 35 19 17 34 30 19\n",
            " 22 21 23 27 27  4 20 19 20 20 21 29]\n",
            "Cummulative reps: [  0  19  43  75  95 119 139 158 189 213 242 262 283 306 339 373 391 418\n",
            " 442 477 496 513 547 577 596 618 639 662 689 716 720 740 759 779 799 820\n",
            " 849]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaKAX2XvGL3O",
        "colab_type": "text"
      },
      "source": [
        "**1.3. Calculate mean and standard deviation of each sensor and each file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaytNKUWGo7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "75e0b05c-2600-4b34-dac7-05c07308b211"
      },
      "source": [
        "# Average and standard deviation of each sensor in each file\n",
        "for s in colnames:\n",
        "  avg=matrix[s].mean(axis=1)\n",
        "  sd=matrix[s].std(axis=1)\n",
        "  sensor=pd.concat([avg.rename(s+': Mean_'),sd.rename('St. dev.')], axis=1)\n",
        "  if s=='EMG0L':\n",
        "    asd=sensor\n",
        "  else:\n",
        "    asd=pd.concat([asd, sensor], axis=1)\n",
        "print('Average and standard deviation of each sensor per file')\n",
        "print(asd)\n",
        "\n",
        "# Average and standard deviation of each file\n",
        "avg=matrix.mean(axis=1)\n",
        "sd=matrix.std(axis=1)\n",
        "print('Average and standard deviation of each file')\n",
        "pd.concat([avg.rename('Mean'),sd.rename('St. dev.')], axis=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average and standard deviation of each sensor per file\n",
            "     EMG0L: Mean_   St. dev.  EMG1L: Mean_  ...   St. dev.  OYR: Mean_   St. dev.\n",
            "0           -2.32   8.664825         -0.80  ...  28.427609       86.70  10.529356\n",
            "1           -1.80   9.544739         -4.08  ...  27.821010       85.68   6.579002\n",
            "2           -3.16  13.085839         -2.32  ...  30.345736       91.88  17.358924\n",
            "3           -0.82  10.123099         -3.16  ...  29.645002       88.46  12.969682\n",
            "4           -0.50   6.516071          2.76  ...  13.237239      116.58  18.099600\n",
            "..            ...        ...           ...  ...        ...         ...        ...\n",
            "844          0.56   7.754288          0.22  ...  17.236281      104.76   7.528341\n",
            "845         -1.74   6.520955         -1.20  ...  21.772891       69.66  85.251036\n",
            "846          1.02   7.731436          0.46  ...  22.707735       56.76  79.408415\n",
            "847          0.06   5.582078          2.36  ...  23.650422       71.62  84.238701\n",
            "848         -2.22   7.434860         -1.62  ...  25.101622       58.52  79.668763\n",
            "\n",
            "[849 rows x 68 columns]\n",
            "Average and standard deviation of each file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean</th>\n",
              "      <th>St. dev.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.256437</td>\n",
              "      <td>46.545832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14.220110</td>\n",
              "      <td>47.316822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15.441784</td>\n",
              "      <td>45.614456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13.651092</td>\n",
              "      <td>47.038916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15.063294</td>\n",
              "      <td>49.105612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>13.167964</td>\n",
              "      <td>41.642351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>13.815314</td>\n",
              "      <td>42.880121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>12.970618</td>\n",
              "      <td>42.725653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>13.852317</td>\n",
              "      <td>43.758631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>13.144035</td>\n",
              "      <td>44.761750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>849 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Mean   St. dev.\n",
              "0    15.256437  46.545832\n",
              "1    14.220110  47.316822\n",
              "2    15.441784  45.614456\n",
              "3    13.651092  47.038916\n",
              "4    15.063294  49.105612\n",
              "..         ...        ...\n",
              "844  13.167964  41.642351\n",
              "845  13.815314  42.880121\n",
              "846  12.970618  42.725653\n",
              "847  13.852317  43.758631\n",
              "848  13.144035  44.761750\n",
              "\n",
              "[849 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1w9SOqkf5vE",
        "colab_type": "text"
      },
      "source": [
        "# **2. Data adjustement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em1GhMP586_j",
        "colab_type": "text"
      },
      "source": [
        "**2.1. Establish equal number of epochs per word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8AHyV4AQGcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(reps)-1,-1,-1):\n",
        "  if reps[i]<numreps:\n",
        "    for r in range(len(matrix)-1,-1,-1):\n",
        "      if int(matrix.iloc[r]['Word'])==i:\n",
        "        matrix=matrix.drop(r)\n",
        "  elif reps[i]>numreps:\n",
        "    elim=random.sample(range(repsum[i],repsum[i+1]),reps[i]-numreps)\n",
        "    matrix=matrix.drop(elim)\n",
        "\n",
        "exec(\"matrix.to_csv(path_or_buf='/content/equal_matrix_'+str(numreps)+'.csv')\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv_VaSCnHJYz",
        "colab_type": "text"
      },
      "source": [
        "**2.2. Create sensor combinatory matrices: unaltered, normalized, and standardized**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Ib41KW5f32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Eliminate unnecessary columns to create combinatory matrices\n",
        "for v in products:\n",
        "  fnword=[]\n",
        "  fsword=[]\n",
        "  m=matrix.copy()\n",
        "  if v%3!=0:\n",
        "    m=m.drop(emg,1)\n",
        "  if v%5!=0:\n",
        "    m=m.drop(acc,1)\n",
        "  if v%7!=0:\n",
        "    m=m.drop(gyro,1)\n",
        "  if v%11!=0:\n",
        "    m=m.drop(ori,1)\n",
        "  \n",
        "  # Separate features from target values\n",
        "  x = m.iloc[:, m.columns!='Word']   # Features\n",
        "\n",
        "  # Create column of words instead of numbers\n",
        "  wordcol=np.empty(len(m), dtype=object)\n",
        "  z=0\n",
        "  wcol=[int(i) for i in m['Word'].values]\n",
        "  for f in wcol:\n",
        "    wordcol[z]=words[f]\n",
        "    z+=1\n",
        "  wordcol=np.asmatrix(wordcol)\n",
        "  regular=np.concatenate((np.transpose(wordcol),np.copy(m.iloc[:,1:])),axis=1)\n",
        "  inputmatrix=pd.DataFrame(regular,columns=m.columns).dropna(axis=1)\n",
        "  regproducts.append(inputmatrix)\n",
        "\n",
        "  for pr in range(len(inputmatrix)):\n",
        "    nemg=[]\n",
        "    nacc=[]\n",
        "    ngyro=[]\n",
        "    nori=[]\n",
        "    norm=[]\n",
        "    semg=[]\n",
        "    sacc=[]\n",
        "    sgyro=[]\n",
        "    sori=[]\n",
        "    stan=[]\n",
        "    if emg[0] in inputmatrix:\n",
        "      in_emg=inputmatrix.iloc[pr][emg]\n",
        "      nemg=(in_emg-in_emg.min())/(in_emg.max()-in_emg.min())\n",
        "      is_emg=inputmatrix.iloc[pr][emg].values\n",
        "      is_emg=is_emg[np.newaxis]\n",
        "      semg=StandardScaler().fit_transform(np.transpose(is_emg))\n",
        "      semg=semg.reshape(-1)\n",
        "    if acc[0] in inputmatrix:\n",
        "      in_acc=inputmatrix.iloc[pr][acc]\n",
        "      nacc=(in_acc-in_acc.min())/(in_acc.max()-in_acc.min())\n",
        "      is_acc=inputmatrix.iloc[pr][acc].values\n",
        "      is_acc=is_acc[np.newaxis]\n",
        "      sacc=StandardScaler().fit_transform(np.transpose(is_acc))\n",
        "      sacc=sacc.reshape(-1)\n",
        "    if gyro[0] in inputmatrix:\n",
        "      in_gyro=inputmatrix.iloc[pr][gyro]\n",
        "      ngyro=(in_gyro-in_gyro.min())/(in_gyro.max()-in_gyro.min())\n",
        "      is_gyro=inputmatrix.iloc[pr][gyro].values\n",
        "      is_gyro=is_gyro[np.newaxis]\n",
        "      sgyro=StandardScaler().fit_transform(np.transpose(is_gyro))\n",
        "      sgyro=sgyro.reshape(-1)\n",
        "    if ori[0] in inputmatrix:\n",
        "      in_ori=inputmatrix.iloc[pr][ori]\n",
        "      nori=(in_ori-in_ori.min())/(in_ori.max()-in_ori.min())\n",
        "      is_ori=inputmatrix.iloc[pr][ori].values\n",
        "      is_ori=is_ori[np.newaxis]\n",
        "      sori=StandardScaler().fit_transform(np.transpose(is_ori))\n",
        "      sori=sori.reshape(-1)\n",
        "    \n",
        "    ## Normalize each type of sensor per sample between 0 and 1\n",
        "    norm=np.concatenate((np.transpose(nemg),np.transpose(nacc),np.transpose(ngyro),np.transpose(nori)))\n",
        "    norm=norm[np.newaxis]\n",
        "    fnword=np.append(fnword,norm)\n",
        "    fnword=fnword[np.newaxis]\n",
        "    \n",
        "    ## Standardize each type of sensor with mean=0 and deviation=1\n",
        "    stan=np.concatenate((np.transpose(semg),np.transpose(sacc),np.transpose(sgyro),np.transpose(sori)))\n",
        "    stan=stan[np.newaxis]\n",
        "    fsword=np.append(fsword,stan)\n",
        "    fsword=fsword[np.newaxis]\n",
        "\n",
        "  nmx=np.reshape(fnword,(len(inputmatrix),len(inputmatrix.columns)-1))\n",
        "  norm_matrix=np.concatenate((np.transpose(wordcol),nmx),axis=1) \n",
        "  norm_matrix=pd.DataFrame(norm_matrix,columns=inputmatrix.columns).dropna(axis=1)\n",
        "  normproducts.append(norm_matrix)\n",
        "  exec(\"norm_matrix.to_csv(path_or_buf='/content/norm_matrix_'+str(numreps)+'.'+str(v)+'.csv')\")\n",
        "\n",
        "  smx=np.reshape(fsword,(len(inputmatrix),len(inputmatrix.columns)-1))\n",
        "  stan_matrix=np.concatenate((np.transpose(wordcol),smx),axis=1) \n",
        "  stan_matrix=pd.DataFrame(stan_matrix,columns=inputmatrix.columns).dropna(axis=1)\n",
        "  stanproducts.append(stan_matrix)\n",
        "  exec(\"stan_matrix.to_csv(path_or_buf='/content/stan_matrix_'+str(numreps)+'.'+str(v)+'.csv')\")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJtV04hjOid6",
        "colab_type": "text"
      },
      "source": [
        "# **3. Definition of functions for steps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQF9gp3gVloy",
        "colab_type": "text"
      },
      "source": [
        "**3.1. Split data into train and test sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfHNiVxNVvK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def datasplit(inmatrix_p):\n",
        "    x = inmatrix_p.iloc[:, inmatrix_p.columns!='Word']   # Features\n",
        "    y = inmatrix_p.loc[:,'Word']                         # Target\n",
        "    x_train_p, x_test_p, y_train_p, y_test_p = train_test_split(x, y, test_size=0.3)\n",
        "    return x_train_p, x_test_p, y_train_p, y_test_p"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGLs1pC8Vg15",
        "colab_type": "text"
      },
      "source": [
        "**3.2. Butterworth**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJjGC_1Q5r03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def butterworth(inmatrix_b):\n",
        "  fs_nyq=50/2\n",
        "  b, a = sp.signal.butter(8, 20/fs_nyq, btype='highpass', fs=fs_nyq)\n",
        "  if emg[0] in inmatrix_b:\n",
        "    for r in range(len(inmatrix_b)):\n",
        "      in_emg=inmatrix_b.iloc[r][emg].values\n",
        "      emg_filtered = sp.signal.lfilter(b, a, in_emg)\n",
        "      emg_filtered=emg_filtered[np.newaxis]\n",
        "      inmatrix_b.iloc[r,1:len(np.transpose(emg_filtered))+1]=emg_filtered\n",
        "  return inmatrix_b"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxsIrFj9V7SX",
        "colab_type": "text"
      },
      "source": [
        "**3.3. PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7WTmphBWFnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pca(x_train_c, x_test_c, y_train_c, y_test_c):\n",
        "  pca = PCA(n_components=min(len(x_train_c), len(y_train_c)))\n",
        "  pca.fit(x_train_c)\n",
        "  x_t_train_pca = pca.transform(x_train_c)\n",
        "  x_t_test_pca = pca.transform(x_test_c)\n",
        "  return x_train_c, x_test_c, y_train_c, y_test_c, x_t_train_pca, x_t_test_pca"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrXL60g9WO_M",
        "colab_type": "text"
      },
      "source": [
        "**3.4. SVM with Grid Search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz0otyHbWRRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svm(x_train_s, x_test_s, y_train_s, y_test_s, x_t_train_s, x_t_test_s,combo):\n",
        "    parameters = {'kernel':('linear', 'poly', 'rbf', 'sigmoid'), 'C':[0.1, 1, 10, 100, 1000]}\n",
        "    svc = SVC(max_iter=1000)\n",
        "    nested_scores=np.zeros(num_trials)\n",
        "    for t in range(num_trials):\n",
        "      inner_cv=KFold(n_splits=10,shuffle=True,random_state=i)\n",
        "      outer_cv=KFold(n_splits=10,shuffle=True,random_state=i)\n",
        "      clf=GridSearchCV(svc,parameters,scoring='accuracy',n_jobs=1,cv=outer_cv)\n",
        "      clf.fit(x_t_train_s, y_train_s)\n",
        "      nested_scores=cross_val_score(clf,x_t_test_s,y_test_s,cv=outer_cv)\n",
        "      nested_scores[i]=nested_scores.mean()\n",
        "    y_pred=clf.predict(x_t_test_s)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    plot_confusion_matrix(clf, x_t_test_s, y_test_s, cmap=plt.cm.Blues)\n",
        "    plt.title(combo)\n",
        "    plt.show()\n",
        "\n",
        "    bestpar=clf.best_params_\n",
        "    accuracy=nested_scores.mean()\n",
        "    svmresult=classification_report(y_test_s, y_pred)\n",
        "    return svmresult,accuracy, bestpar"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIoQ54eagg4r",
        "colab_type": "text"
      },
      "source": [
        "# **4. Main code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKoS0bOpWV7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for st in steps:\n",
        "  rownum+=1\n",
        "  columnnum=-1\n",
        "  for pr in regproducts:\n",
        "    columnnum+=1\n",
        "    combo=nsteps[rownum]+' - '+comb[columnnum]\n",
        "    words=set(pr['Word']) # Obtain the target names for the SVM\n",
        "    \n",
        "    # First step: set input matrix as regular, normalized or standardized\n",
        "    input_matrix=pr\n",
        "    if st%3==0: # Normalization\n",
        "      input_matrix=normproducts[columnnum]\n",
        "    if st%5==0: # Standardization\n",
        "      input_matrix=standproducts[columnnum]\n",
        "    \n",
        "    # Second step: apply Butterworth\n",
        "    two_matrix=input_matrix\n",
        "    if st%7==0: # Butterworth\n",
        "      two_matrix=butterworth(input_matrix)\n",
        "    \n",
        "    # Third step: split data for later steps\n",
        "    x_train, x_test, y_train, y_test=datasplit(two_matrix)\n",
        "    \n",
        "    # Fourth step: apply PCA\n",
        "    x_t_train=x_train\n",
        "    x_t_test=x_test\n",
        "    if st%11==0: # PCA\n",
        "      x_train, x_test, y_train, y_test, x_t_train, x_t_test=pca(x_train, x_test, y_train, y_test)\n",
        "    \n",
        "    # Fifth step: apply SVM\n",
        "    if st%13==0: # SVM\n",
        "      svmresults,accuracy,par=svm(x_train, x_test, y_train, y_test, x_t_train, x_t_test,combo)\n",
        "    \n",
        "    print('The best parameters for',combo,'are:',par)\n",
        "    fresults[rownum,columnnum]=accuracy*100\n",
        "fresults=pd.DataFrame(fresults,index=nsteps,columns=comb)\n",
        "exec(\"fresults.to_csv(path_or_buf='/content/results_'+str(numreps)+'.csv')\")\n",
        "\n",
        "endtime=time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PwH5XRYrIqd",
        "colab_type": "text"
      },
      "source": [
        "# **5. Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqoHNGBxrMHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(fresults)\n",
        "print('The maximum accuracy for',numreps,'repetitions is',fresults.max().max())\n",
        "print('The program executed in '+str(endtime-starttime)+' seconds')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}